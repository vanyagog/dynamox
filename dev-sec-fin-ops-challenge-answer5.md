
# DevOps Analysis

## Процесс выпуска новой версии сервисов

1. **Разработка и коммиты**  
   Разработчик вносит изменения в код и пушит их в основную ветку репозитория (например, `main`).

2. **Continuous Integration (CI)**  
   При каждом пуше запускается CI-пайплайн, который выполняет:  
   - Проверку качества кода (линтеры, тесты).  
   - Сборку Docker-образов для backend и extractor сервисов.  
   - Проверку успешности сборки.

3. **Continuous Deployment (CD)**  
   После успешного CI выполняется CD-процесс, включающий:  
   - Тэгирование и публикацию Docker-образов в реестр (Docker Hub, GitHub Container Registry или приватный).  
   - Обновление Kubernetes-манифестов с новыми тегами образов.  
   - Деплой в тестовое/staging окружение.  
   - Автоматическое или ручное тестирование.

4. **Релиз в production**  
   После успешного тестирования обновления продвигаются в production, с мониторингом и оповещениями о статусе.

## Автоматизация: CI/CD Pipeline

Для автоматизации процесса предлагается использовать GitHub Actions с таким пайплайном:

```yaml
name: CI/CD Pipeline

on:
  push:
    branches:
      - main

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    env:
      DOCKER_REGISTRY: ghcr.io
      IMAGE_BACKEND: ${{ env.DOCKER_REGISTRY }}/your-org/backend-counter
      IMAGE_EXTRACTOR: ${{ env.DOCKER_REGISTRY }}/your-org/extractor
    steps:
      - uses: actions/checkout@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Login to Docker Registry
        uses: docker/login-action@v2
        with:
          registry: ${{ env.DOCKER_REGISTRY }}
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Build and push backend image
        working-directory: ./backend
        run: |
          docker build -t $IMAGE_BACKEND:${{ github.sha }} .
          docker push $IMAGE_BACKEND:${{ github.sha }}

      - name: Build and push extractor image
        working-directory: ./extractor
        run: |
          docker build -t $IMAGE_EXTRACTOR:${{ github.sha }} .
          docker push $IMAGE_EXTRACTOR:${{ github.sha }}

      - name: Deploy to Kubernetes
        uses: appleboy/kubectl-action@v0.1.9
        with:
          kubeconfig: ${{ secrets.KUBE_CONFIG }}
          args: apply -f k8s/
```

### Комментарии

- В переменных окружения и секретах хранится доступ к Docker Registry и Kubernetes.  
- Образы помечаются тегом коммита для удобства отслеживания.  
- Манифесты Kubernetes можно параметризовать для использования этих тегов, например с помощью Helm или Kustomize.

## Рекомендации по улучшению

- Добавить автоматическое тестирование в пайплайн (юнит, интеграционные тесты).  
- Использовать Canary или Blue-Green деплойменты для минимизации рисков.  
- Настроить мониторинг и алертинг (Prometheus, Grafana, Slack).  
- Организовать ручное подтверждение релиза в production, если требуется.

---


---
# SecOps Analysis

## Потенциальные риски безопасности

| Риск                                         | Описание                                                                                  |
|----------------------------------------------|-------------------------------------------------------------------------------------------|
| 1. Неаутентифицированный доступ к API        | API backend доступен без аутентификации, что позволяет любому получить данные и исказить статистику. |
| 2. Отсутствие шифрования трафика             | Трафик между клиентом и сервисом передаётся без HTTPS, что может привести к перехвату и изменению данных. |
| 3. Уязвимости в сторонних библиотеках        | Используемые библиотеки Python могут содержать известные уязвимости.                       |
| 4. Недостаточная изоляция контейнеров        | Контейнеры могут иметь избыточные привилегии и возможности взаимодействия.                 |
| 5. Хранение секретов и паролей в открытом виде| Секреты и пароли могут быть неправильно сохранены, что увеличивает риск компрометации.    |
| 6. Недостаточный мониторинг и логирование     | Отсутствие централизованного логирования и мониторинга усложняет обнаружение инцидентов. |
| 7. Возможные DoS-атаки                         | Отсутствие ограничений на число запросов может привести к перегрузке сервиса.             |
| 8. Небезопасные права доступа в Kubernetes   | Избыточные права доступа могут привести к эскалации привилегий внутри кластера.           |

## Меры по снижению рисков

- Внедрить аутентификацию и авторизацию API (например, OAuth2, JWT или API-ключи).  
- Настроить HTTPS/TLS для всех внешних и внутренних коммуникаций (Ingress с сертификатами Let's Encrypt).  
- Регулярно обновлять зависимости и использовать инструменты сканирования уязвимостей (Snyk, Dependabot).  
- Настроить политики безопасности контейнеров (Pod Security Policies, seccomp, AppArmor).  
- Хранить секреты только в Kubernetes Secrets или специализированных системах управления секретами (HashiCorp Vault).  
- Внедрить централизованное логирование и мониторинг с оповещениями (ELK, Prometheus, Grafana, Alertmanager).  
- Ввести rate limiting и защиту от DoS на уровне API Gateway или Ingress контроллера.  
- Минимизировать права доступа в Kubernetes с помощью RBAC и принципа наименьших привилегий.

## Рекомендации по реализации

- Использовать встроенную поддержку OAuth2/JWT в FastAPI для защиты API.  
- Настроить Ingress контроллер с TLS и базовой аутентификацией.  
- Добавить проверку безопасности зависимостей в CI/CD пайплайн.  
- Ограничить права сервис-аккаунтов Kubernetes, используемых приложениями.  
- Настроить мониторинг и оповещения на подозрительную активность.

---


# FinOps Analysis

## Оценка стоимости сервисов на Google Cloud

### Входные данные

| Атрибут        | Backend Deployment       | Extraction Cronjob         |
|----------------|-------------------------|----------------------------|
| Machine Type   | n1-highcpu-4            | n1-highmem-2               |
| Number of Pods | 55                      | 28                         |
| CPU            | 1.25                    | 0.5                        |
| Memory         | 512 Mi (0.5 Gi)         | 2 Gi                       |

### Примерные цены (ориентировочно)

| Машина         | Цена в час (USD)         |
|----------------|--------------------------|
| n1-highcpu-4   | 0.60                     |
| n1-highmem-2   | 0.38                     |

---

### Расчёт Backend Deployment

- CPU на все поды: 55 × 1.25 = 68.75 CPU  
- Память: 55 × 0.5 Gi = 27.5 Gi  
- Машина n1-highcpu-4 имеет 4 CPU и 3.6 Gi памяти  
- Требуется минимум 18 таких машин (по CPU)  
- Стоимость в час: 18 × 0.60 = 10.8 USD  
- Стоимость в день: 10.8 × 24 = 259.2 USD  
- Стоимость за 30 дней: 259.2 × 30 = 7,776 USD  
- Стоимость за 365 дней: 259.2 × 365 = 94,608 USD  

### Расчёт Extraction Cronjob

- CPU на все поды: 28 × 0.5 = 14 CPU  
- Память: 28 × 2 Gi = 56 Gi  
- Машина n1-highmem-2 имеет 2 CPU и 13 Gi памяти  
- Требуется минимум 7 таких машин (по CPU)  
- Стоимость в час: 7 × 0.38 = 2.66 USD  
- Стоимость в день: 2.66 × 24 = 63.84 USD  
- Стоимость за 30 дней: 63.84 × 30 = 1,915.2 USD  
- Стоимость за 365 дней: 63.84 × 365 = 23,301.6 USD  

---

### Итоговая оценка затрат

| Сервис             | 30 дней (USD) | 365 дней (USD) |
|--------------------|---------------|----------------|
| Backend Deployment  | 7,776         | 94,608         |
| Extraction Cronjob  | 1,915.2       | 23,301.6       |
| **Всего**          | **9,691.2**   | **117,909.6**  |

---

### Рекомендации по оптимизации затрат

- Запускать тестовые окружения с меньшим количеством подов.  
- Использовать preemptible инстансы (spot instances).  
- Автоматически масштабировать нагрузку с помощью HPA.  
- Оптимизировать использование CPU и памяти.  
- Рассмотреть альтернативные типы машин и регионы.

---
